# -*- coding: utf-8 -*-
"""UCMerced_linear_feature_eval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1866eTCYkCpZd_9XeIJ54MatLGHhtBGsF
"""



import torch
import sys
import yaml
from torchvision import transforms, datasets
import torchvision
import numpy as np
import os
import provider
import importlib
from tqdm import tqdm
from sklearn import preprocessing
from torch.utils.data.dataloader import DataLoader
from data_utils.ModelNetDataLoader import ModelNetDataLoader
from models.resnet_base_network import ResNet18
from models.pointnet2_cls_msg import get_model
from models.pointnet2_cls_msg import get_loss



# train_dataset = datasets.STL10('/home/thalles/Downloads/', split='train', download=False,
#                                transform=data_transforms)

# test_dataset = datasets.STL10('/home/thalles/Downloads/', split='test', download=False,
#                                transform=data_transforms)

from torchvision import datasets
from models.mlp_head import MLPHead
from models.resnet_base_network import ResNet18





class LogisticRegression(torch.nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LogisticRegression, self).__init__()
        self.linear = torch.nn.Linear(input_dim, output_dim)
        
    def forward(self, x):
        return self.linear(x)



def get_features_from_encoder(encoder, loader):
    
    x_train = []
    y_train = []
    print(type(loader))
    # get the features from the pre-trained model
    for batch_id, data in tqdm(enumerate(loader, 0), total=len(loader), smoothing=0.9):
        
           points, target = data
           points = points.data.numpy()
           points = provider.random_point_dropout(points)
           points[:,:, 0:3] = provider.random_scale_point_cloud(points[:,:, 0:3])
           points[:,:, 0:3] = provider.shift_point_cloud(points[:,:, 0:3])
           points = torch.Tensor(points)
           target = target[:, 0]
           points = points.transpose(2, 1)
           points, target = points.cuda(), target.cuda()
           with torch.no_grad():
                feature_vector = encoder(points)
                # print("feature_vector.size()",len(feature_vector))
                # print(np.ndim(feature_vector))
                x_train.extend(feature_vector)
                y_train.extend(target.cpu().numpy())
    # x_train = torch.stack(x_train)
    y_train = torch.tensor(y_train)
    print("success feature")


    # for i, (x,y) in enumerate(loader):
    #     # i=i.to(device)
    #     # x=x.to(device)
    #     # y=y.to(device)
    #     x1=torch.tensor([item.cpu().detach().numpy() for item in x1]).cuda() 
    #     with torch.no_grad():
    #         feature_vector = encoder(x1)
    #         x_train.extend(feature_vector)
    #         y_train.extend(y.numpy())

            
    # x_train = torch.stack(x_train)
    # y_train = torch.tensor(y_train)
    return x_train, y_train



def create_data_loaders_from_arrays(X_train, y_train, X_test, y_test):

    train = torch.utils.data.TensorDataset(X_train, y_train)
    train_loader = torch.utils.data.DataLoader(train, batch_size=6, shuffle=True)

    test = torch.utils.data.TensorDataset(X_test, y_test)
    test_loader = torch.utils.data.DataLoader(test, batch_size=6, shuffle=False)
    return train_loader, test_loader



def get_acc(pred1,target,trans_feat1):
 batch_size = 8

 config = yaml.load(open("/content/PointNet-BYOL/config/config.yaml", "r"), Loader=yaml.FullLoader)

#  TRAIN_DATASET = ModelNetDataLoader(root='data/modelnet40_normal_resampled/', npoint=1024, split='train',
#                                                   normal_channel=False)
#  TEST_DATASET = ModelNetDataLoader(root='data/modelnet40_normal_resampled/', npoint=1024, split='test',
#                                                  normal_channel=False)


#  print("Input shape:", len(TRAIN_DATASET))

#  train_loader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=16, shuffle=True, num_workers=4)
#  test_loader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=16, shuffle=False, num_workers=4)

#  print(type(train_loader))

 device = 'cuda' if torch.cuda.is_available() else 'cpu' #'cuda' if torch.cuda.is_available() else 'cpu'
#  encoder = get_model(num_class=40,normal_channel=True)
#  output_feature_dim = encoder.projetion.net[0].in_features# 



 #load pre-trained parameters
#  load_params = torch.load(os.path.join('/content/PointNet-BYOL/checkpoints/model.pth'),
#                           map_location=torch.device(torch.device(device)))

#  if 'online_network_state_dict' in load_params:
#      encoder.load_state_dict(load_params['online_network_state_dict'])
#      print("Parameters successfully loaded.")

 # remove the projection head
#  encoder = encoder.to(device)
#  encoder.eval()
 
 logreg = LogisticRegression(128, 40)
 logreg = logreg.to(device)
 
#  x_train, y_train = get_features_from_encoder(encoder, trainDataLoader)
#  x_test, y_test = get_features_from_encoder(encoder, testDataLoader)

#  if len(x_train.shape) > 2:
#      x_train = torch.mean(x_train, dim=[2, 3])
#      x_test = torch.mean(x_test, dim=[2, 3])
     
#  print("Training data shape:", x_train.shape, y_train.shape)
#  print("Testing data shape:", x_test.shape, y_test.shape)
 
#  scaler = preprocessing.StandardScaler()
#  scaler.fit(x_train.cpu())
#  x_train = scaler.transform(x_train.cpu()).astype(np.float32)
#  x_test = scaler.transform(x_test.cpu()).astype(np.float32)

#  train_loader, test_loader = create_data_loaders_from_arrays(torch.tensor([item.cpu().detach().numpy() for item in x_train]).cuda() , y_train, torch.from_numpy(x_test), y_test)
 optimizer = torch.optim.Adam(logreg.parameters(), lr=3e-4)
 criterion = get_loss()
 eval_every_n_epochs = 40
 train = torch.utils.data.TensorDataset(pred1, target)
 train_loader = torch.utils.data.DataLoader(train, batch_size=6, shuffle=True)
 for epoch in range(200):
#     train_acc = []
    for x, y in train_loader:
        x = x.to(device)
        y = y.to(device)
        # zero the parameter gradients
        optimizer.zero_grad()        
      
        logits = logreg(x)
        predictions = torch.argmax(logits, dim=1)
        loss = criterion(logits, y.long(),y)
        loss.backward(retain_graph=True)
        optimizer.step()
    
    
    if epoch % eval_every_n_epochs == 0:
        train_total,total = 0,0
        train_correct,correct = 0,0
        for x, y in train_loader:
            x = x.to(device)
            y = y.to(device)

            logits = logreg(x)
            predictions = torch.argmax(logits, dim=1)
            
            train_total += y.size(0)
            train_correct += (predictions == y).sum().item()
        # for x, y in test_loader:
        #     x = x.to(device)
        #     y = y.to(device)

        #     logits = logreg(x)
        #     predictions = torch.argmax(logits, dim=1)
            
        #     total += y.size(0)
        #     correct += (predictions == y).sum().item()
        train_acc=  train_correct / train_total 
        # acc =  correct / total
        print(f"Training accuracy: {np.mean(train_acc)}")
        # print(f"Testing accuracy: {np.mean(acc)}")
 return train_acc
