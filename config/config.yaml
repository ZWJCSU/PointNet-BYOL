network:
  name: MLP
  # Specify a folder containing a pre-trained model to fine-tune. If training from scratch, pass None.
  fine_tune_from: '../log/classification/pointnet2_cls_msg/checkpoints/'
  projection_head:
    mlp_hidden_size: 512
    projection_size: 40

data_transforms:
  s: 1
  input_shape: (96,96,3)

trainer:
  batch_size: 32
  m: 0.996 # momentum update
  checkpoint_interval: 5000
  max_epochs: 10
  num_workers: 4

optimizer:
  params:
    lr: 0.03
    momentum: 0.9
    weight_decay: 0.0004
